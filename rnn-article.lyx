#LyX 2.4 created this file. For more info see https://www.lyx.org/
\lyxformat 553
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
%the following commands are used only for book or software reviews

%\Reviewer{Some Author\\University of Somewhere}
%\Plainreviewer{Some Author}

%the following commands are used for articles, codesnippets, book reviews and software reviews

%publication information
%do not use these commands before the article has been accepted
%\Volume{00}
%\Issue{0}
%\Month{Month}
%\Year{2000}
%\Submitdate{2000-00-00}
%\Acceptdate{2000-00-00}

%if you use Sweave,  include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}
\end_preamble
\options article
\use_default_options false
\begin_modules
knitr
logicalmkup
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding auto
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format pdf2
\output_sync 0
\bibtex_command default
\index_command default
\float_placement class
\float_alignment class
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_title "rnn: Recurrent Neural Network architectures in native R"
\pdf_author "Bastiaan Quast"
\pdf_keywords "recurrent neural network, R package, R language"
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine biblatex
\cite_engine_type authoryear
\biblio_style plainnat
\biblatex_bibstyle authoryear
\biblatex_citestyle authoryear
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\begin_inset Flex Code
status open

\begin_layout Plain Layout
rnn
\end_layout

\end_inset

: Recurrent Neural Network architectures
\begin_inset Newline newline
\end_inset

in native 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
R
\end_layout

\end_inset


\end_layout

\begin_layout Author
Bastiaan Quast
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Research supported by the Swiss National Science Foundation
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

International Telecommuniation Union 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
and 
\end_layout

\end_inset

Dimitri Fichou
\begin_inset Newline newline
\end_inset

University of Giessen
\end_layout

\begin_layout Abstract
\noindent
The R package 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
rnn
\end_layout

\end_inset

 implements several Recurrent Neural Network (RNN) architectures in the
 R language.
 The native 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
R
\end_layout

\end_inset

 implementations of these architectures allow scientists familiar with the
 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
R
\end_layout

\end_inset

 language, to develop an intuitive understanding of these architectures,
 something which is not possible with production frameworks, such as TensorFlow,
 PyTorch or CNTK.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
mention website: 
\begin_inset CommandInset href
LatexCommand href
target "http://qua.st/rnn"

\end_inset


\end_layout

\begin_layout Plain Layout
fix Digital Ocean droplet, setup demo again
\end_layout

\end_inset


\end_layout

\begin_layout Section
About package 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
rnn
\end_layout

\end_inset

 in 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
R
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
rnn
\end_layout

\end_inset

 package is available on 
\begin_inset CommandInset href
LatexCommand href
name "CRAN"
target "https://cran.r-project.org/"
literal "false"

\end_inset

 at 
\begin_inset CommandInset href
LatexCommand href
target "https://cran.r-project.org/package=rnn"

\end_inset

 and can be installed using
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
The development version can be installed using 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
devtools::install_github('bquast/rnn')
\end_layout

\end_inset


\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

install, eval=FALSE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

install.packages('rnn')
\end_layout

\end_inset


\end_layout

\begin_layout Standard
After installation, the package can be loaded using:
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

load-package
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

library(rnn)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The following functions are exported by the package.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

list-functions
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

ls('package:rnn')
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A list of all the functions - including non-exported ones - is shown below.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

list-all-functions
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

ls(getNamespace('rnn'), all.names=TRUE)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
rnn
\end_layout

\end_inset

 package has one dependency, the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
sigmoid
\end_layout

\end_inset

 package 
\begin_inset CommandInset citation
LatexCommand citep
key "quast2016sigmoid"
literal "false"

\end_inset

, which is on 
\begin_inset CommandInset href
LatexCommand href
name "CRAN"
target "https://cran.r-project.org/"
literal "false"

\end_inset

 at 
\begin_inset CommandInset href
LatexCommand href
target "https://cran.r-project.org/package=sigmoid"

\end_inset

.
 The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
sigmoid
\end_layout

\end_inset

 package provides a collection of sigmoid functions such as the Rectified
 Linear Unit (
\begin_inset Flex Code
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
name "ReLU()"
target "https://cran.r-project.org/web/packages/sigmoid/vignettes/sigmoid.html#relu"
literal "false"

\end_inset


\end_layout

\end_inset

), 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
name "Gompertz()"
target "https://cran.r-project.org/web/packages/sigmoid/vignettes/sigmoid.html#gompertz"
literal "false"

\end_inset


\end_layout

\end_inset

, etc.
 Until version 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
0.8.0
\end_layout

\end_inset

 of the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
rnn
\end_layout

\end_inset

 package, the sigmoid functions were included in the package, after which
 they were released as a separate package for more general use.
\end_layout

\begin_layout Standard
In addition to this, the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
rnn
\end_layout

\end_inset

 package includes a 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Shiny
\end_layout

\end_inset

 app
\begin_inset Note Note
status open

\begin_layout Plain Layout
need to remove as dependency
\end_layout

\end_inset

 demonstrating a Recurrent Neural Network analysis of a time series (Foreign
 Exchange rates).
 In order to run the app locally, the 
\begin_inset CommandInset href
LatexCommand href
name "Shiny package"
target "https://cran.r-project.org/package=shiny"
literal "false"

\end_inset

 needs to be installed.
\end_layout

\begin_layout Section
trainr()
\end_layout

\begin_layout Standard
The workhorse of the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
rnn
\end_layout

\end_inset

 package is the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
trainr()
\end_layout

\end_inset

 function, it trains a model based on input and output data, given the specified
 hyperparameters.
\end_layout

\begin_layout Standard
The documentation of the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
trainr()
\end_layout

\end_inset

 function can be called up using:
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

help_trainr, eval=FALSE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

help('trainr')
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Recurrent Neural Networks 
\begin_inset CommandInset citation
LatexCommand citep
key "werbos1988generalization"
literal "false"

\end_inset

 have the ability to learn bit-by-bit binary addition (including carrying
 over) with as little as 3 hidden nodes, whereas feed-forward neural networks
 would need many more.
\end_layout

\begin_layout Standard
First training data is generated, the training data is between 0-127, or
 an 8-bit binary.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

training-input, cache=TRUE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

set.seed(123) # for reproducible random numbers
\end_layout

\begin_layout Plain Layout

X1 = sample(0:127, 50000, replace=TRUE)
\end_layout

\begin_layout Plain Layout

X2 = sample(0:127, 50000, replace=TRUE)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The training data is used to generate the output data or labels.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

training-output, cache=TRUE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

Y <- X1 + X2
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Following this, both the input data and the output data are converted into
 binary format, using the built-in 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
int2bin()
\end_layout

\end_inset

 function.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

int2bin, cache=TRUE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

X1 <- int2bin(X1, length=8)
\end_layout

\begin_layout Plain Layout

X2 <- int2bin(X2, length=8)
\end_layout

\begin_layout Plain Layout

Y  <- int2bin(Y,  length=8)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Finally, the two input variables are stored in a single 3 dimensional tensor.
 Where the first dimension contains observations, the second dimension time,
 and the third dimension variables.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

tensor, cache=TRUE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

X <- array( c(X1,X2), dim=c(dim(X1),2) )
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The objects 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
X
\end_layout

\end_inset

 and 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Y
\end_layout

\end_inset

 can now be fed to the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
trainr()
\end_layout

\end_inset

 function, which will output a trained model (stored here in the object
 called 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
m1
\end_layout

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

train, cache=TRUE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

m1 <- trainr(Y=Y,
\end_layout

\begin_layout Plain Layout

             X=X,
\end_layout

\begin_layout Plain Layout

             learningrate   = 1,
\end_layout

\begin_layout Plain Layout

             hidden_dim     = 6   )
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The forward and back propagation algorithms used are relatively straightforward
 and can be printed.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

forward-prop, eval=FALSE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

rnn:::predict_rnn
\end_layout

\begin_layout Plain Layout

rnn:::backprop_rnn
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A simplied version of the rnn algorithm used is presented in the vignette
 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
basic_rnn
\end_layout

\end_inset

, also available on CRAN at 
\begin_inset CommandInset href
LatexCommand href
target "https://cran.r-project.org/package=rnn/vignettes/basic_rnn.html"

\end_inset

, it is also included in Appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Simplied-RNN-code"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

vignette, eval=FALSE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

vignette('basic_rnn')
\end_layout

\end_inset


\end_layout

\begin_layout Section
predictr()
\end_layout

\begin_layout Standard
Using a trained model to make predictions is done using the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
predictr()
\end_layout

\end_inset

 function.
\end_layout

\begin_layout Standard
Observations 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
1
\end_layout

\end_inset

 & 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
2
\end_layout

\end_inset

 of the training data, the variable 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
1
\end_layout

\end_inset

, in binary format.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

testing1
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

X[1:2,,1]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The same observations 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
1
\end_layout

\end_inset

 & 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
2
\end_layout

\end_inset

; variable 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
1
\end_layout

\end_inset

, this time in decimal format.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

testing1-decimal
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

bin2int( X[1:2,,1] )
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Observations 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
1
\end_layout

\end_inset

 & 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
2
\end_layout

\end_inset

 of the training data, the input variable 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
2
\end_layout

\end_inset

, in binary format.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

testing2
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

X[1:2,,2]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Observations 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
1
\end_layout

\end_inset

 & 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
2
\end_layout

\end_inset

; variable 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
2
\end_layout

\end_inset

, in decimal format.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

testing2-decimal
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

bin2int( X[1:2,,2] )
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Summing observation 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
1
\end_layout

\end_inset

 of variable 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
1
\end_layout

\end_inset

: 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
36
\end_layout

\end_inset

, with observation 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
1
\end_layout

\end_inset

 of variable 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
2
\end_layout

\end_inset

: 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
119
\end_layout

\end_inset

, gives 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
155
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Summing observation 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
2
\end_layout

\end_inset

 of variable 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
1
\end_layout

\end_inset

: 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
100
\end_layout

\end_inset

, with observation 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
2
\end_layout

\end_inset

 of variable 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
2
\end_layout

\end_inset

: 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
75
\end_layout

\end_inset

, gives 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
175
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Make predictions using the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
predictr()
\end_layout

\end_inset

 function.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

predict
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

round( predictr(model = m1,
\end_layout

\begin_layout Plain Layout

                X     = X[1:2,,] ) )
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Compared to the ground truth values.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

ground-truth
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

Y[1:2,]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Or in decimal format.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

predict-dec
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

bin2int(round( predictr(model = m1,
\end_layout

\begin_layout Plain Layout

                        X     = X[1:2,,] ) ) )
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Compared to the ground truth values.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

ground-truth-dec
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

bin2int( Y[1:2,] )
\end_layout

\end_inset


\end_layout

\begin_layout Section
Architectures
\end_layout

\begin_layout Standard
In addition to fully-connected Recurrent Neural Networks, 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
rnn
\end_layout

\end_inset

 also supports Long Short-Term Memory (LSTM) Recurrent Neural Networks 
\begin_inset CommandInset citation
LatexCommand citep
key "hochreiter1997long"
literal "false"

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

lstm, eval=FALSE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

trainr(Y, X, network_type=
\begin_inset Quotes erd
\end_inset

lstm
\begin_inset Quotes erd
\end_inset

)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
as well as Gated Recurrent Unit 
\begin_inset CommandInset citation
LatexCommand citep
key "cho2014learning"
literal "false"

\end_inset

 architecture.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

gru, eval=FALSE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

trainr(Y, X, network_type=
\begin_inset Quotes erd
\end_inset

gru
\begin_inset Quotes erd
\end_inset

)
\end_layout

\end_inset


\end_layout

\begin_layout Section
Miscellaneous Functions
\end_layout

\begin_layout Standard
In addition to the main user APIs 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
trainr()
\end_layout

\end_inset

 and 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
predictr()
\end_layout

\end_inset

, the underlying functions for the above mentioned neural network architectures,
 the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
rnn
\end_layout

\end_inset

 package includes several other functions.
 Including the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
int2bin()
\end_layout

\end_inset

 and 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
bin2int()
\end_layout

\end_inset

 which were used above, these funtions are intended to ease conversion from
 decimal to binary notation and visa versa, especially for didacting purposes.
\end_layout

\begin_layout Standard
The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
run.rnn_demo()
\end_layout

\end_inset

 and 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
run.finance_demo()
\end_layout

\end_inset

 each launch a 
\begin_inset CommandInset href
LatexCommand href
name "Shiny app"
target "http://shiny.rstudio.com/"
literal "false"

\end_inset

 that demonstate usage of 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
rnn
\end_layout

\end_inset

 functionality, the Shiny package needs to be installed in order to run
 these apps locally
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
The Shiny package can be installed using: 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
install.packages('shiny')
\end_layout

\end_inset


\end_layout

\end_inset

.
 The two demos are also available online at 
\begin_inset CommandInset href
LatexCommand href
target "http://shiny.qua.st/rnn"

\end_inset

 and 
\begin_inset CommandInset href
LatexCommand href
target "http://shiny.qua.st/finance"

\end_inset

 respectively.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
nocite{hebb2005organization}
\end_layout

\begin_layout Plain Layout


\backslash
nocite{linnainmaa1970representation}
\end_layout

\begin_layout Plain Layout


\backslash
nocite{mcculloch1943logical}
\end_layout

\begin_layout Plain Layout


\backslash
nocite{minsky1952neural}
\end_layout

\begin_layout Plain Layout


\backslash
nocite{rosenblatt1958perceptron}
\end_layout

\begin_layout Plain Layout


\backslash
nocite{rumelhart1986learning}
\end_layout

\begin_layout Plain Layout


\backslash
nocite{werbos1974beyond}
\end_layout

\begin_layout Plain Layout


\backslash
nocite{werbos1988generalization}
\end_layout

\begin_layout Plain Layout


\backslash
nocite{widrow1960adaptive}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "rnn-article"
options "plainnat"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
appendix
\end_layout

\end_inset


\end_layout

\begin_layout Section
Simplied RNN code
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:Simplied-RNN-code"

\end_inset


\end_layout

\begin_layout Standard
Decimal to binary conversion.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

i2b, cache=TRUE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

i2b <- function(integer, length=8)
\end_layout

\begin_layout Plain Layout

  as.numeric(intToBits(integer))[1:length]
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# apply to entire vectors
\end_layout

\begin_layout Plain Layout

int2bin <- function(integer, length=8)
\end_layout

\begin_layout Plain Layout

  t(sapply(integer, i2b, length=length)) 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Training data generation.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

training_data_generation, cache=TRUE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

# set training data length
\end_layout

\begin_layout Plain Layout

training_data_size = 20000
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# create sample inputs 
\end_layout

\begin_layout Plain Layout

X1 = sample(0:127, training_data_size, replace=TRUE) 
\end_layout

\begin_layout Plain Layout

X2 = sample(0:127, training_data_size, replace=TRUE)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# create sample output 
\end_layout

\begin_layout Plain Layout

Y <- X1 + X2
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# convert to binary 
\end_layout

\begin_layout Plain Layout

X1 <- int2bin(X1) 
\end_layout

\begin_layout Plain Layout

X2 <- int2bin(X2) 
\end_layout

\begin_layout Plain Layout

Y  <- int2bin(Y)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# create 3d array: dim 1: samples; dim 2: time; dim 3: variables 
\end_layout

\begin_layout Plain Layout

X <- array( c(X1,X2), dim=c(dim(X1),2) )
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Sigmoid and derivative functions.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

sig_and_der, cache=TRUE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

sigmoid <- function(x)
\end_layout

\begin_layout Plain Layout

             1 / ( 1+exp(-x) )
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

sig_to_der <- function(x)
\end_layout

\begin_layout Plain Layout

                x*(1-x)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Set the hyperparameters.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

hyperparameters, cache=TRUE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

binary_dim = 8 
\end_layout

\begin_layout Plain Layout

alpha      = 0.5 
\end_layout

\begin_layout Plain Layout

input_dim  = 2 
\end_layout

\begin_layout Plain Layout

hidden_dim = 6 
\end_layout

\begin_layout Plain Layout

output_dim = 1
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Initialise the weights.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

init_weights_rnn, cache=TRUE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

# initialize weights randomly between -1 and 1, with mean 0
\end_layout

\begin_layout Plain Layout

weights_0 = matrix(runif(n = input_dim *hidden_dim, min=-1, max=1),
\end_layout

\begin_layout Plain Layout

                   nrow=input_dim,
\end_layout

\begin_layout Plain Layout

                   ncol=hidden_dim ) 
\end_layout

\begin_layout Plain Layout

weights_h = matrix(runif(n = hidden_dim*hidden_dim, min=-1, max=1),
\end_layout

\begin_layout Plain Layout

                   nrow=hidden_dim,
\end_layout

\begin_layout Plain Layout

                   ncol=hidden_dim )
\end_layout

\begin_layout Plain Layout

weights_1 = matrix(runif(n = hidden_dim*output_dim, min=-1, max=1),
\end_layout

\begin_layout Plain Layout

                   nrow=hidden_dim,
\end_layout

\begin_layout Plain Layout

                   ncol=output_dim ) 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# create matrices to store updates, to be used in backpropagation
\end_layout

\begin_layout Plain Layout

weights_0_update = matrix(0, nrow = input_dim,  ncol = hidden_dim) 
\end_layout

\begin_layout Plain Layout

weights_h_update = matrix(0, nrow = hidden_dim, ncol = hidden_dim)
\end_layout

\begin_layout Plain Layout

weights_1_update = matrix(0, nrow = hidden_dim, ncol = output_dim)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Train the model.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

train_rnn, cache=TRUE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

# training logic
\end_layout

\begin_layout Plain Layout

for (j in 1:training_data_size) {
\end_layout

\begin_layout Plain Layout

    # select data
\end_layout

\begin_layout Plain Layout

    a = X1[j,]
\end_layout

\begin_layout Plain Layout

    b = X2[j,]
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

    # select true answer
\end_layout

\begin_layout Plain Layout

    c = Y[j,]
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

    # where we'll store our best guesss (binary encoded)
\end_layout

\begin_layout Plain Layout

    d = matrix(0, nrow = 1, ncol = binary_dim)
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

    overallError = 0
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

    layer_2_deltas = matrix(0)
\end_layout

\begin_layout Plain Layout

    layer_1_values = matrix(0, nrow=1, ncol = hidden_dim)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    # moving along the positions in the binary encoding
\end_layout

\begin_layout Plain Layout

    for (position in 1:binary_dim) {
\end_layout

\begin_layout Plain Layout

        # generate input and output
\end_layout

\begin_layout Plain Layout

        X = cbind( a[position], b[position] )
\end_layout

\begin_layout Plain Layout

        y = c[position]
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

        # hidden layer
\end_layout

\begin_layout Plain Layout

        layer_1 = sigmoid( (X%*%weights_0) +
\end_layout

\begin_layout Plain Layout

                    (layer_1_values[dim(layer_1_values)[1],]
\end_layout

\begin_layout Plain Layout

                                               %*% weights_h) )
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

        # output layer
\end_layout

\begin_layout Plain Layout

        layer_2 = sigmoid(layer_1 %*% weights_1)
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

        # did we miss?...
 if so, by how much?
\end_layout

\begin_layout Plain Layout

        layer_2_error = y - layer_2
\end_layout

\begin_layout Plain Layout

        layer_2_deltas = rbind(layer_2_deltas,
\end_layout

\begin_layout Plain Layout

                               layer_2_error * sig_to_der(layer_2))
\end_layout

\begin_layout Plain Layout

        overallError = overallError + abs(layer_2_error)
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

        # decode estimate so we can print it out
\end_layout

\begin_layout Plain Layout

        d[position] = round(layer_2)
\end_layout

\begin_layout Plain Layout

        
\end_layout

\begin_layout Plain Layout

        # store hidden layer
\end_layout

\begin_layout Plain Layout

        layer_1_values = rbind(layer_1_values, layer_1)
\end_layout

\begin_layout Plain Layout

    }
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    future_layer_1_delta = matrix(0, nrow = 1, ncol = hidden_dim)
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

    for (position in binary_dim:1) {
\end_layout

\begin_layout Plain Layout

        X = cbind(a[position], b[position])
\end_layout

\begin_layout Plain Layout

        layer_1 = layer_1_values[dim(layer_1_values)[1]
\end_layout

\begin_layout Plain Layout

                                      - (binary_dim-position),]
\end_layout

\begin_layout Plain Layout

        prev_layer_1 = layer_1_values[dim(layer_1_values)[1]
\end_layout

\begin_layout Plain Layout

                                      - ( (binary_dim-position)+1 ),]
\end_layout

\begin_layout Plain Layout

        
\end_layout

\begin_layout Plain Layout

        # error at output layer
\end_layout

\begin_layout Plain Layout

        layer_2_delta = layer_2_deltas[dim(layer_2_deltas)[1] - 
\end_layout

\begin_layout Plain Layout

                                           (binary_dim-position),]
\end_layout

\begin_layout Plain Layout

        # error at hidden layer
\end_layout

\begin_layout Plain Layout

        layer_1_delta = (future_layer_1_delta %*% t(weights_h) +
\end_layout

\begin_layout Plain Layout

          layer_2_delta %*% t(weights_1)) * sig_to_der(layer_1)
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

        # let's update all our weights so we can try again
\end_layout

\begin_layout Plain Layout

        weights_1_update = weights_1_update + matrix(layer_1) %*% 
\end_layout

\begin_layout Plain Layout

                                                      layer_2_delta
\end_layout

\begin_layout Plain Layout

        weights_h_update = weights_h_update + matrix(prev_layer_1) %*%
\end_layout

\begin_layout Plain Layout

                                                      layer_1_delta
\end_layout

\begin_layout Plain Layout

        weights_0_update = weights_0_update + t(X) %*% layer_1_delta
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

        future_layer_1_delta = layer_1_delta
\end_layout

\begin_layout Plain Layout

    }
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

    weights_0 = weights_0 + ( weights_0_update * alpha )
\end_layout

\begin_layout Plain Layout

    weights_1 = weights_1 + ( weights_1_update * alpha )
\end_layout

\begin_layout Plain Layout

    weights_h = weights_h + ( weights_h_update * alpha )
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

    weights_0_update = weights_0_update * 0
\end_layout

\begin_layout Plain Layout

    weights_1_update = weights_1_update * 0
\end_layout

\begin_layout Plain Layout

    weights_h_update = weights_h_update * 0
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

    if(j%%(training_data_size/5) == 0)
\end_layout

\begin_layout Plain Layout

        print(paste("Error:", overallError))    
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_body
\end_document
